{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMbWyPRDob6ub5fnSobUig8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aBHg8Kycxukm","executionInfo":{"status":"ok","timestamp":1741270035604,"user_tz":-480,"elapsed":2106,"user":{"displayName":"Nehemiah Damian","userId":"07331916233452330666"}},"outputId":"f18bdcdf-7396-43c5-a622-da27c7a06755"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20 - Losses: {'ner': 54.485787093639374}\n","Epoch 2/20 - Losses: {'ner': 27.353702729567885}\n","Epoch 3/20 - Losses: {'ner': 10.540911356103607}\n","Epoch 4/20 - Losses: {'ner': 5.637465596708353}\n","Epoch 5/20 - Losses: {'ner': 2.565196139589176}\n","Epoch 6/20 - Losses: {'ner': 2.378807262755622}\n","Epoch 7/20 - Losses: {'ner': 1.5543455640811206}\n","Epoch 8/20 - Losses: {'ner': 0.9157243171780509}\n","Epoch 9/20 - Losses: {'ner': 0.8549427399761483}\n","Epoch 10/20 - Losses: {'ner': 0.38745614373732135}\n","Epoch 11/20 - Losses: {'ner': 0.22792139969362446}\n","Epoch 12/20 - Losses: {'ner': 0.006607355316795445}\n","Epoch 13/20 - Losses: {'ner': 0.00026305856594251615}\n","Epoch 14/20 - Losses: {'ner': 4.816760939088345e-06}\n","Epoch 15/20 - Losses: {'ner': 7.0799974626530915e-06}\n","Epoch 16/20 - Losses: {'ner': 8.446102089718254e-07}\n","Epoch 17/20 - Losses: {'ner': 9.966883433403513e-07}\n","Epoch 18/20 - Losses: {'ner': 5.714352987015685e-07}\n","Epoch 19/20 - Losses: {'ner': 5.720361505562692e-07}\n","Epoch 20/20 - Losses: {'ner': 2.0926395981335552e-07}\n","\n","--- Testing on Training Data ---\n","\n","Sentence: mayra gi dengue ka aron mauwasan nang imong giatay na nawong   mauwasan diay ang nawng kung dengueon ow\n","Entity: dengue, Label: SAKET\n","Entity: dengueon, Label: SAKET\n","\n","Sentence: so kapila gid diay makakua ug dengue ang isa ka tao kay kung n ko karon pang anim na to ba\n","Entity: dengue, Label: SAKET\n","Entity: tao, Label: PERSON\n","\n","Sentence: may dengue gid man kung muni tsura ka lamok\n","Entity: lamok, Label: HAYOP\n","\n","Sentence: natakdan gid ko dengue ba\n","Entity: dengue, Label: SAKET\n","\n","Sentence: natakdan gid ko dengue ba\n","Entity: dengue, Label: SAKET\n"]}],"source":["\n","#FINAL CODE\n","import spacy\n","from spacy.training import Example\n","import random\n","\n","\n","training_data = {\n","    \"classes\": [\"ORG\", \"PERSON\", \"SAKET\", \"BAHAY\", \"HAYOP\"],\n","    \"annotations\": [\n","        [\"mayra gi dengue ka aron mauwasan nang imong giatay na nawong   mauwasan diay ang nawng kung dengueon ow\", {\"entities\": [[9,15,\"SAKET\"], [92,100,\"SAKET\"]]}],\n","        [\"so kapila gid diay makakua ug dengue ang isa ka tao kay kung n ko karon pang anim na to ba\", {\"entities\": [[30,36,\"SAKET\"], [48,51,\"PERSON\"]]}],\n","        [\"may dengue gid man kung muni tsura ka lamok\", {\"entities\": [[38,43,\"HAYOP\"]]}],\n","        [\"natakdan gid ko dengue ba\", {\"entities\": [[16,22,\"SAKET\"]]}],\n","        [\"natakdan gid ko dengue ba\", {\"entities\": [[16,22,\"SAKET\"]]}]\n","    ]\n","}\n","\n","\n","nlp = spacy.blank(\"xx\")\n","\n","if \"ner\" not in nlp.pipe_names:\n","    ner = nlp.add_pipe(\"ner\", last=True)\n","else:\n","    ner = nlp.get_pipe(\"ner\")\n","\n","for label in training_data[\"classes\"]:\n","    ner.add_label(label)\n","\n","train_examples = []\n","for text, annotations in training_data[\"annotations\"]:\n","    doc = nlp.make_doc(text)\n","    example = Example.from_dict(doc, annotations)\n","    train_examples.append(example)\n","\n","optimizer = nlp.initialize()\n","\n","n_epochs = 20\n","for epoch in range(n_epochs):\n","    random.shuffle(train_examples)\n","    losses = {}\n","    for example in train_examples:\n","        nlp.update([example], sgd=optimizer, losses=losses)\n","    print(f\"Epoch {epoch + 1}/{n_epochs} - Losses: {losses}\")\n","\n","print(\"\\n--- Testing on Training Data ---\")\n","for text, _ in training_data[\"annotations\"]:\n","    doc = nlp(text)\n","    print(\"\\nSentence:\", text)\n","    for ent in doc.ents:\n","        print(f\"Entity: {ent.text}, Label: {ent.label_}\")\n"]},{"cell_type":"code","source":[],"metadata":{"id":"e3sEb1KsxvxB"},"execution_count":null,"outputs":[]}]}